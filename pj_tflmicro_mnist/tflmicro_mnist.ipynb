{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tflmicro_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4GhlFxbHHPc9VmDmspo/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iwatake2222/pico-work/blob/master/pj_tflmicro_mnist/tflmicro_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h_Uw-0UW2rT"
      },
      "source": [
        "# Create models for TensorFlow Lite for Microcontrollers\r\n",
        "- Create the original model using Keras (conv_mnist.h5)\r\n",
        "- Convert the model into TensorFlow Lite\r\n",
        "  - Without quantization (conv_mnist.tflite)\r\n",
        "  - With quantization (conv_mnist_quant.tflite)\r\n",
        "- Convert tflite model into C code using xxd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7oUNMtyX7dd"
      },
      "source": [
        "## Create the original model using Keras (conv_mnist.h5)\r\n",
        "Simple MNIST model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJM-mC2_UFyz",
        "outputId": "75b58bfe-5ebf-4519-8cac-3d11fdad9e34"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "print(tf.__version__)\r\n",
        "\r\n",
        "# MNISTの学習用データ、テストデータをロードする\r\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\n",
        "\r\n",
        "# 学習データの前処理\r\n",
        "# X: 6000x28x28x1のTensorに変換し、値を-1.0～1.0に正規化\r\n",
        "# Y: one-hot化(6000x1 -> 6000x10)   -> no need when using SparseCategoricalCrossentropy()\r\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\r\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\r\n",
        "x_train = x_train / 128. - 1.\r\n",
        "x_test = x_test / 128. - 1.\r\n",
        "x_train = x_train.astype(np.float32)\r\n",
        "x_test = x_test.astype(np.float32)\r\n",
        "# y_train = tf.keras.utils.to_categorical(y_train, 10)\r\n",
        "# y_test = tf.keras.utils.to_categorical(y_test, 10)\r\n",
        "\r\n",
        "# 学習状態把握用のTensorBoard設定\r\n",
        "tsb = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\r\n",
        "\r\n",
        "# Convolutionモデルの作成\r\n",
        "input = tf.keras.layers.Input(shape=(28,28,1), name=\"input_image\")\r\n",
        "conv1 = tf.keras.layers.Conv2D(\r\n",
        "  filters=8,\r\n",
        "  kernel_size=(3,3),\r\n",
        "  strides=(1,1),\r\n",
        "  padding=\"same\",\r\n",
        "  activation=\"relu\"\r\n",
        ")(input)\r\n",
        "pool1 = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(conv1)\r\n",
        "conv2 = tf.keras.layers.Conv2D(\r\n",
        "  filters=4,\r\n",
        "  kernel_size=(3,3),\r\n",
        "  strides=(1,1),\r\n",
        "  padding=\"same\",\r\n",
        "  activation=\"relu\"\r\n",
        ")(pool1)\r\n",
        "dropout1 = tf.keras.layers.Dropout(0.2)(conv2)\r\n",
        "flatten1 = tf.keras.layers.Flatten()(dropout1)\r\n",
        "output = tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"output_scores\")(flatten1)\r\n",
        "model = tf.keras.models.Model(inputs=[input], outputs=[output], name=\"ConvMnist\")\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(\r\n",
        "  optimizer=tf.keras.optimizers.Adam(),\r\n",
        "  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\r\n",
        "  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\r\n",
        ")\r\n",
        "\r\n",
        "# Convolutionモデルの学習\r\n",
        "model.fit(\r\n",
        "  x_train,\r\n",
        "  y_train,\r\n",
        "  batch_size=32,\r\n",
        "  epochs=20,\r\n",
        "  validation_split=0.2,\r\n",
        "  callbacks=[tsb],\r\n",
        ")\r\n",
        "\r\n",
        "# 学習したモデルを使用して、テスト用データで評価する\r\n",
        "model.evaluate(x_test,  y_test, verbose=2)\r\n",
        "\r\n",
        "# 学習済みモデルをファイル(h5)に保存する\r\n",
        "model.save(\"conv_mnist.h5\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Model: \"ConvMnist\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_image (InputLayer)     [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 28, 28, 8)         80        \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 14, 14, 4)         292       \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 4)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "output_scores (Dense)        (None, 10)                7850      \n",
            "=================================================================\n",
            "Total params: 8,222\n",
            "Trainable params: 8,222\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.6656 - sparse_categorical_accuracy: 0.7864 - val_loss: 0.1338 - val_sparse_categorical_accuracy: 0.9638\n",
            "Epoch 2/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1446 - sparse_categorical_accuracy: 0.9564 - val_loss: 0.0836 - val_sparse_categorical_accuracy: 0.9767\n",
            "Epoch 3/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1025 - sparse_categorical_accuracy: 0.9676 - val_loss: 0.0728 - val_sparse_categorical_accuracy: 0.9795\n",
            "Epoch 4/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0869 - sparse_categorical_accuracy: 0.9732 - val_loss: 0.0748 - val_sparse_categorical_accuracy: 0.9785\n",
            "Epoch 5/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0782 - sparse_categorical_accuracy: 0.9753 - val_loss: 0.0643 - val_sparse_categorical_accuracy: 0.9812\n",
            "Epoch 6/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0775 - sparse_categorical_accuracy: 0.9770 - val_loss: 0.0617 - val_sparse_categorical_accuracy: 0.9837\n",
            "Epoch 7/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0728 - sparse_categorical_accuracy: 0.9778 - val_loss: 0.0615 - val_sparse_categorical_accuracy: 0.9827\n",
            "Epoch 8/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0639 - sparse_categorical_accuracy: 0.9796 - val_loss: 0.0608 - val_sparse_categorical_accuracy: 0.9818\n",
            "Epoch 9/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0676 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.0631 - val_sparse_categorical_accuracy: 0.9822\n",
            "Epoch 10/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0651 - sparse_categorical_accuracy: 0.9803 - val_loss: 0.0578 - val_sparse_categorical_accuracy: 0.9837\n",
            "Epoch 11/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0595 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0591 - val_sparse_categorical_accuracy: 0.9838\n",
            "Epoch 12/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0556 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.0576 - val_sparse_categorical_accuracy: 0.9839\n",
            "Epoch 13/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0539 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0573 - val_sparse_categorical_accuracy: 0.9838\n",
            "Epoch 14/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9825 - val_loss: 0.0612 - val_sparse_categorical_accuracy: 0.9825\n",
            "Epoch 15/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0590 - sparse_categorical_accuracy: 0.9813 - val_loss: 0.0560 - val_sparse_categorical_accuracy: 0.9842\n",
            "Epoch 16/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0556 - sparse_categorical_accuracy: 0.9818 - val_loss: 0.0566 - val_sparse_categorical_accuracy: 0.9841\n",
            "Epoch 17/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0522 - sparse_categorical_accuracy: 0.9836 - val_loss: 0.0607 - val_sparse_categorical_accuracy: 0.9828\n",
            "Epoch 18/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0552 - sparse_categorical_accuracy: 0.9827 - val_loss: 0.0561 - val_sparse_categorical_accuracy: 0.9853\n",
            "Epoch 19/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0523 - sparse_categorical_accuracy: 0.9830 - val_loss: 0.0527 - val_sparse_categorical_accuracy: 0.9848\n",
            "Epoch 20/20\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0471 - sparse_categorical_accuracy: 0.9854 - val_loss: 0.0544 - val_sparse_categorical_accuracy: 0.9847\n",
            "313/313 - 1s - loss: 0.0441 - sparse_categorical_accuracy: 0.9855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9kHhF_iYjVg"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "u8gK9ChRU8bH",
        "outputId": "962e0ca1-5007-4ea1-e563-a3cb89739d75"
      },
      "source": [
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Read input image\r\n",
        "!wget -O 4.jpg  \"https://drive.google.com/uc?export=download&id=1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A\" \r\n",
        "img_org = cv2.imread(\"4.jpg\")\r\n",
        "cv2_imshow(img_org)\r\n",
        "\r\n",
        "# Pre process\r\n",
        "## グレースケール化、リサイズ、白黒判定、価範囲を0～255 -> -1.0～1.0\r\n",
        "img = cv2.cvtColor(img_org, cv2.COLOR_BGR2GRAY)\r\n",
        "img = cv2.resize(img, (28, 28))\r\n",
        "img = 255 - img\r\n",
        "img = img / 128. - 1.\r\n",
        "img = img.astype(np.float32)\r\n",
        "input_tensor = img.reshape(1, img.shape[0], img.shape[1], 1)\r\n",
        "\r\n",
        "# Load model\r\n",
        "loaded_model = tf.keras.models.load_model(\"conv_mnist.h5\")\r\n",
        "\r\n",
        "# Inference\r\n",
        "scores = loaded_model.predict(input_tensor)\r\n",
        "\r\n",
        "result = np.argmax(scores[0])\r\n",
        "print(\"predicted number is {} [{:.2f}]\".format(result, scores[0][result]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-14 16:54:53--  https://drive.google.com/uc?export=download&id=1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.102, 74.125.142.113, 74.125.142.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-7s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4qub1roodu7jv7isu3jjqag0iu96gcuh/1613321625000/06137305555544994122/*/1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-14 16:54:53--  https://doc-0c-7s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4qub1roodu7jv7isu3jjqag0iu96gcuh/1613321625000/06137305555544994122/*/1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A?e=download\n",
            "Resolving doc-0c-7s-docs.googleusercontent.com (doc-0c-7s-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0c-7s-docs.googleusercontent.com (doc-0c-7s-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2971 (2.9K) [image/jpeg]\n",
            "Saving to: ‘4.jpg’\n",
            "\n",
            "4.jpg               100%[===================>]   2.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-14 16:54:54 (145 MB/s) - ‘4.jpg’ saved [2971/2971]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F42D4A7AB38>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted number is 4 [0.97]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJZwN0G8YKmm"
      },
      "source": [
        "## Convert the model into TensorFlow Lite without quantization (conv_mnist.tflite)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPHtInQ9VHA9",
        "outputId": "44920a7e-a185-4117-ee9a-e6a64dd22593"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Convert\r\n",
        "loaded_model = tf.keras.models.load_model(\"conv_mnist.h5\")\r\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\r\n",
        "\r\n",
        "tflite_model = converter.convert()\r\n",
        "open(\"conv_mnist.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpojf78kbl/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpojf78kbl/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35396"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yldLzffCYnOt"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "aIAH9XAaVMgm",
        "outputId": "a42dbeda-57e5-46cb-f683-a2d9eabcb867"
      },
      "source": [
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Read input image\r\n",
        "!wget -O 4.jpg  \"https://drive.google.com/uc?export=download&id=1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A\" \r\n",
        "img = cv2.imread(\"4.jpg\")\r\n",
        "cv2_imshow(img)\r\n",
        "\r\n",
        "# Pre process\r\n",
        "## グレースケール化、リサイズ、白黒判定、価範囲を0～255 -> -1.0～1.0\r\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n",
        "img = cv2.resize(img, (28, 28))\r\n",
        "img = 255 - img\r\n",
        "img = img / 128. - 1.\r\n",
        "img = img.astype(np.float32)\r\n",
        "input_tensor = img.reshape(1, img.shape[0], img.shape[1], 1)\r\n",
        "input_tensor = tf.convert_to_tensor(input_tensor)\r\n",
        "\r\n",
        "# Load model\r\n",
        "interpreter = tf.lite.Interpreter(model_path=\"conv_mnist.tflite\")\r\n",
        "# interpreter = tf.lite.Interpreter(model_path=\"conv_mnist_from_pb.tflite\")\r\n",
        "interpreter.allocate_tensors()\r\n",
        "input_details = interpreter.get_input_details()\r\n",
        "output_details = interpreter.get_output_details()\r\n",
        "\r\n",
        "# set input tensor\r\n",
        "interpreter.set_tensor(input_details[0]['index'], input_tensor)\r\n",
        "\r\n",
        "# Inference\r\n",
        "interpreter.invoke()\r\n",
        "\r\n",
        "scores = interpreter.get_tensor(output_details[0]['index'])\r\n",
        "result = np.argmax(scores[0])\r\n",
        "print(\"predicted number is {} [{:.2f}]\".format(result, scores[0][result]))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-14 16:55:15--  https://drive.google.com/uc?export=download&id=1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.102, 74.125.142.113, 74.125.142.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-7s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bu9bs8o00f7iqhc6ai42cusdc7tjthgd/1613321700000/06137305555544994122/*/1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-14 16:55:15--  https://doc-0c-7s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bu9bs8o00f7iqhc6ai42cusdc7tjthgd/1613321700000/06137305555544994122/*/1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A?e=download\n",
            "Resolving doc-0c-7s-docs.googleusercontent.com (doc-0c-7s-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0c-7s-docs.googleusercontent.com (doc-0c-7s-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2971 (2.9K) [image/jpeg]\n",
            "Saving to: ‘4.jpg’\n",
            "\n",
            "4.jpg               100%[===================>]   2.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-14 16:55:16 (151 MB/s) - ‘4.jpg’ saved [2971/2971]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F4360139588>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted number is 4 [0.97]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLe4fjMIYWdc"
      },
      "source": [
        "## Convert the model into TensorFlow Lite with quantization (conv_mnist_quant.tflite)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGgtt9oJVeR4",
        "outputId": "2f08880e-4aeb-4b57-ad29-ae17545bd9c0"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "print(tf.__version__)\r\n",
        "\r\n",
        "## Prepara dataset generator for calibration\r\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\r\n",
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\r\n",
        "x_train = x_train / 128.0 - 1\r\n",
        "x_train = x_train.astype(np.float32)\r\n",
        "num_calibration_images = 100\r\n",
        "calibration_indexes   = np.random.choice(x_train.shape[0], num_calibration_images, replace=False)\r\n",
        "def representative_dataset_gen():\r\n",
        "  for i in range(num_calibration_images):\r\n",
        "    yield [x_train[calibration_indexes[i: i + 1]]]\r\n",
        "\r\n",
        "\r\n",
        "# Convert\r\n",
        "loaded_model = tf.keras.models.load_model(\"conv_mnist.h5\")\r\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\r\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model_file(\"conv_mnist.h5\")\r\n",
        "\r\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\r\n",
        "converter.representative_dataset = representative_dataset_gen\r\n",
        "\r\n",
        "# For full integer quantization\r\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\r\n",
        "## NOT UINT8 but INT8\r\n",
        "# converter.inference_input_type = tf.uint8\r\n",
        "# converter.inference_output_type = tf.uint8\r\n",
        "converter.inference_input_type = tf.int8\r\n",
        "converter.inference_output_type = tf.int8\r\n",
        "converter.experimental_new_converter = True   # will be no need in the future\r\n",
        "\r\n",
        "tflite_model = converter.convert()\r\n",
        "open(\"conv_mnist_quant.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "INFO:tensorflow:Assets written to: /tmp/tmpu0yzrj3a/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpu0yzrj3a/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11832"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CryB8RcrYpDr"
      },
      "source": [
        "### Test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "GMAmfkZKVs-u",
        "outputId": "e65e4ee9-6d6c-4d46-9f6e-1cdbbbe2716d"
      },
      "source": [
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Read input image\r\n",
        "!wget -O 4.jpg  \"https://drive.google.com/uc?export=download&id=1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A\" \r\n",
        "img = cv2.imread(\"4.jpg\")\r\n",
        "cv2_imshow(img)\r\n",
        "\r\n",
        "# Pre process\r\n",
        "## グレースケール化、リサイズ、白黒判定、価範囲を-128～127\r\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n",
        "img = cv2.resize(img, (28, 28))\r\n",
        "img = (255 - img) - 128\r\n",
        "\r\n",
        "# img = img.astype(np.uint8)\r\n",
        "img = img.astype(np.int8)\r\n",
        "input_tensor = img.reshape(1, img.shape[0], img.shape[1], 1)\r\n",
        "input_tensor = tf.convert_to_tensor(input_tensor)\r\n",
        "\r\n",
        "# Load model\r\n",
        "interpreter = tf.lite.Interpreter(model_path=\"conv_mnist_quant.tflite\")\r\n",
        "interpreter.allocate_tensors()\r\n",
        "input_details = interpreter.get_input_details()\r\n",
        "output_details = interpreter.get_output_details()\r\n",
        "\r\n",
        "# set input tensor\r\n",
        "interpreter.set_tensor(input_details[0]['index'], input_tensor)\r\n",
        "\r\n",
        "# Inference\r\n",
        "interpreter.invoke()\r\n",
        "\r\n",
        "scores = interpreter.get_tensor(output_details[0]['index'])\r\n",
        "result = np.argmax(scores[0])\r\n",
        "\r\n",
        "print(\"predicted number is {} [{:.2f}]\".format(result, scores[0][result]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-14 17:01:51--  https://drive.google.com/uc?export=download&id=1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.101, 74.125.142.102, 74.125.142.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0c-7s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ittbnpkc0m4894otjsel4c8i3d5scjj4/1613322075000/06137305555544994122/*/1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-14 17:01:51--  https://doc-0c-7s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ittbnpkc0m4894otjsel4c8i3d5scjj4/1613322075000/06137305555544994122/*/1-3yb3qCrN8M6Bdj7ZZ9UMjONh34R2W_A?e=download\n",
            "Resolving doc-0c-7s-docs.googleusercontent.com (doc-0c-7s-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
            "Connecting to doc-0c-7s-docs.googleusercontent.com (doc-0c-7s-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2971 (2.9K) [image/jpeg]\n",
            "Saving to: ‘4.jpg’\n",
            "\n",
            "4.jpg               100%[===================>]   2.90K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-14 17:01:51 (147 MB/s) - ‘4.jpg’ saved [2971/2971]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=128x128 at 0x7F436033D0B8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predicted number is 4 [121.00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dO0LicNYr0W"
      },
      "source": [
        "## Convert tflite model into C code using xxd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PtGb4gLV5c9",
        "outputId": "26a549a2-efb1-421d-cdb8-6bea9380f21e"
      },
      "source": [
        "!sudo apt install xxd"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xxd\n",
            "0 upgraded, 1 newly installed, 0 to remove and 17 not upgraded.\n",
            "Need to get 49.3 kB of archives.\n",
            "After this operation, 200 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 xxd amd64 2:8.0.1453-1ubuntu1.4 [49.3 kB]\n",
            "Fetched 49.3 kB in 1s (87.3 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xxd.\n",
            "(Reading database ... 146425 files and directories currently installed.)\n",
            "Preparing to unpack .../xxd_2%3a8.0.1453-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking xxd (2:8.0.1453-1ubuntu1.4) ...\n",
            "Setting up xxd (2:8.0.1453-1ubuntu1.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-__RSEzTWCNm",
        "outputId": "acee0b2b-3b84-450c-a5ca-554bef804b2a"
      },
      "source": [
        "!xxd -i conv_mnist_quant.tflite > conv_mnist_quant.cpp\r\n",
        "!xxd -i conv_mnist.tflite > conv_mnist.cpp\r\n",
        "!ls -la"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 904\n",
            "drwxr-xr-x 1 root root   4096 Feb 14 16:50 .\n",
            "drwxr-xr-x 1 root root   4096 Feb 14 16:13 ..\n",
            "-rw-r--r-- 1 root root   2971 Feb 14 16:58 4.jpg\n",
            "drwxr-xr-x 1 root root   4096 Feb 10 14:40 .config\n",
            "drwxr-xr-x 2 root root   4096 Feb 14 16:50 conv_mnist\n",
            "-rw-r--r-- 1 root root 218360 Feb 14 16:59 conv_mnist.cpp\n",
            "-rw-r--r-- 1 root root 135568 Feb 14 16:54 conv_mnist.h5\n",
            "-rw-r--r-- 1 root root  73060 Feb 14 16:59 conv_mnist_quant.cpp\n",
            "-rw-r--r-- 1 root root  11832 Feb 14 16:55 conv_mnist_quant.tflite\n",
            "-rw-r--r-- 1 root root  35396 Feb 14 16:55 conv_mnist.tflite\n",
            "-rw-r--r-- 1 root root 411142 Feb 14 16:50 conv_mnist.zip\n",
            "drwxr-xr-x 4 root root   4096 Feb 14 16:14 logs\n",
            "drwxr-xr-x 1 root root   4096 Feb 10 14:40 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "1PtlL6DWWU44",
        "outputId": "de2452f4-d675-423e-ad99-f14e5b9ac077"
      },
      "source": [
        "!mkdir conv_mnist\r\n",
        "!cp conv_mnist*.* conv_mnist/.\r\n",
        "!zip conv_mnist.zip -r conv_mnist\r\n",
        "from google.colab import files\r\n",
        "files.download( \"./conv_mnist.zip\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘conv_mnist’: File exists\n",
            "updating: conv_mnist/ (stored 0%)\n",
            "updating: conv_mnist/conv_mnist_quant.cpp (deflated 81%)\n",
            "updating: conv_mnist/conv_mnist.cpp (deflated 76%)\n",
            "updating: conv_mnist/conv_mnist_quant.tflite (deflated 29%)\n",
            "updating: conv_mnist/conv_mnist.h5 (deflated 29%)\n",
            "updating: conv_mnist/conv_mnist.tflite (deflated 10%)\n",
            "updating: conv_mnist/conv_mnist.zip (stored 0%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_76fd3083-e89c-4590-9470-0b592198a062\", \"conv_mnist.zip\", 616267)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}